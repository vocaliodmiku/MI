/*
 * File: LM.cpp.
 * Last modified at 19:51 on 21 December 2013 by WuBin.
 * Last modified at 18:05 on 9 January 2013 by WuBin. - Add facility to compute sentence probability.
 *
 * The implementation of the LM class.
 * This class provides basic facilities of the LM by reading the ARPA format file generated by the SRILM toolkit.
 */

#include "LM.h"
using namespace std;
/* -----------------------------------------Initialize--------Maps---------------------------------------------------- */
/* The constructor. */
LM::LM(int order, string ARPAFile) : ORDER(order), ARPAFormatFile(ARPAFile) {
	cerr << "Loading "<< ARPAFormatFile << "..." << endl;
	if (ORDER != 2 && ORDER != 3) cerr << "Error: LM::LM: It only supports the bigram and the trigram. "<< endl;
	ifstream in(ARPAFormatFile.c_str());
	if (in.fail()) cerr << "Error: LM::LM: Opening a file failed. " << endl;

	initUnigramWithBOW(in);
	if (ORDER == 2) initBigram(in);
	if (ORDER == 3) {
		initBigramWithBOW(in);
		initTrigram(in);
	}

	string end;	in >> end;
	std::cout << "END is " << end;		
	cerr << "LM::LM ORDER: " << ORDER << "." << endl << endl;
}

/* Read unigrams with their probabilities and backoff weights. 
 * Words of </s> or <s> are filtered. (For both the bigram model and the trigram model.) 
 */
void LM::initUnigramWithBOW(ifstream &in){
	/* The standrard format of a line of a unigram with its backoff weight and probability: 
	 * prob		word  	  bow */
	Log10Prob pr; 	string word; 	Log10BOW bow;

	/* Delete garbage lines.
	 * \1-grams:
     * -0.8750613	</s>  -- NOTE: Expect for this line, all lines of unigrams are in the standard format. 
	 */
	string dummyToken;
	while (dummyToken != "\\1-grams:") in >> dummyToken;
	in >> dummyToken; in >> dummyToken;

	/* Lines with the standard format. */
	while (in >> pr >> word >> bow) {
		if (word != "<s>" && word != "</s>") {
			GramInfo gramInfo(pr, bow);
			unigramWithBOW.insert(make_pair(word,gramInfo));
		}
	}

	in.clear();
	cerr << "LM::initUnigramWithBOW done!" << endl;
}

/* Read bigrams with their probabilities.  (For the bigram model.) 
 * A gram with word or history of </s> or <s> is filtered. 
 */
void LM::initBigram(ifstream &in) {
    /* The standrard format of a line of bigram: 
	 * prob		history		word */
	Log10Prob pr; string history; string word;

	string header; in >> header;
	assert(header == "\\2-grams:");

	while (in >> pr >> history >> word) {
		/* Add the item to bigram map. */
		if (history != "</s>" && history != "<s>" && word != "</s>" && word != "<s>") {
			if (bigram.find(history) == bigram.end()) {
				map<string, Log10Prob> m;
				m.insert(make_pair(word, pr));
				bigram.insert(make_pair(history, m));
			} else {
				/* map.insert() returns pair<map, bool>;
				 * bool would indicate key's existence.
				 * Here we assert all (history, word) are different.
				 */
				assert((bigram[history].insert(make_pair(word, pr))).second);
			}
		}
	}
	in.clear();
	cerr <<  "LM::initBigram done!" << endl;
}

/* Parse a line of a bigram with its bow and its prob. 
 * Format: prob		history		word  	  (bow) 
 * Note: bow may be ommited if not required as a prefix of the higher order gram. 
 */
inline void LM::parseBigramWithBOW(const string &line, Log10Prob &pr, string &history, string &word, Log10BOW &bow) const {
	stringstream converter; converter << line;
	converter >> pr >> history >> word >> bow;
	if (converter.fail()) bow = LOG10_BOW_EMPTY;
}

/* Read bigrams with their probabilities and their backoff weights.  (For the trigram model.) 
 * A gram with word or history of </s> or <s> is filtered. 
 */
void LM::initBigramWithBOW(ifstream &in) {
	/* Standrard format of a line of a bigram with its bow and prob: 
	 * prob		history		word  	  (bow) 
	 * Note: bow may be ommited if not required as prefix of the high-order gram. */
	Log10Prob pr; string history; string word; Log10BOW bow;

	string header; getline(in, header); chomp(header);
	assert(header == "\\2-grams:");

	string line;
	while (true) {
		getline(in, line);	chomp(line);
		if (line.empty()) break;
		parseBigramWithBOW(line, pr, history, word, bow);
		/* Add the item to the bigramWithBOW map. */
		if (history != "</s>" && history != "<s>" && word != "</s>" && word != "<s>") {
			GramInfo info(pr, bow);
			if (bigramWithBOW.find(history) == bigramWithBOW.end()) {
				map<string, GramInfo> m;
				m.insert(make_pair(word, info));
				bigramWithBOW.insert(make_pair(history, m));
			} else {
				/* map.insert() returns pair<map, bool>,
				 * bool would indicate key's existence.
				 * Here we assert all (history, word) are different.
				 */
				assert ((bigramWithBOW[history].insert(make_pair(word, info))).second);
			}
		}
	}
	cerr << "LM::initBigramWithBOW done!" << endl;
}

/* Read trigrams with their probabilities and backoff weights.  (For the trigram model.) 
 * A gram with word or history or distant history of </s> or <s> is filtered. 
 */
void LM::initTrigram(ifstream &in) {
	/* The standrard format of a line of a trigram: 
	 * prob		distantHistory		history		word */
	Log10Prob pr; string dHistory; string history; string word;

	string header; in >> header;
	assert(header == "\\3-grams:");

	while (in >> pr >> dHistory >> history >> word) {
		/* Add the item to trigram map. 
		 * Note: map			key					value
		 *      trigram 		key: dhistory		value: mapOut
		 *      mapOut  	 	key: history		value: mapIn
		 *      mapIn   		key: word			value: probability (or gram info)
		 */
	    if (dHistory != "</s>" && dHistory != "<s>" && history != "</s>" && history != "<s>" && word != "</s>" && word != "<s>") {
			if (trigram.find(dHistory) == trigram.end()) {
				map<string, Log10Prob> mIn;
				mIn.insert(make_pair(word, pr));
				map<string, map<string, Log10Prob> > mOut;
				mOut.insert(make_pair(history, mIn));
				trigram.insert(make_pair(dHistory, mOut));
			} else if (trigram[dHistory].find(history) == trigram[dHistory].end()) {
				map<string, Log10Prob> mIn;
				mIn.insert(make_pair(word, pr));
				trigram[dHistory].insert(make_pair(history, mIn));
			} else {
				/* Assume all (dhistory, history, word) tuples are different. */
				assert(trigram[dHistory][history].insert(make_pair(word, pr)).second);
			}
		}
	}
	in.clear();
	cerr << "LM::initTrigram done!" << endl;
}

/* -----------------------------------------Search--------Functions---------------------------------------------------- */

/* Return true if finding the unigram, storing its backoff weight and probability information, false otherwise. */
bool LM::findUnigramWithBOW(const string &word, Log10Prob &pr, Log10Prob &bow) const {
	UnigramWithBOW::const_iterator itr = unigramWithBOW.find(word);
	if (itr != unigramWithBOW.end()) {
		pr = itr->second.pr;
		bow = itr->second.bow;
		return true;
	} else {
		/* The word is out of vocabulary (OOV), 
		 * so the probability of the word is 0, 
		 * and backoff weight of the word as history of a bigram is also 0. 
		 */ 
		pr = LOG10_PROB_ZERO;
		bow = LOG10_BOW_ZERO;
    	return false;
	}
}

/* Given a bigram, if it exists, return its probability, otherwise return LOG10_PROB_ZERO. */
bool LM::findBigram(const string &history, const string &word, Log10Prob &pr) const {
	bool found = true;
	map<string, Log10Prob>::const_iterator it;
	
	/* Get the existence of gram in the bigram map. */
	Bigram::const_iterator itr = bigram.find(history);
	if (itr  == bigram.end()) {
		found = false;
	} else {
		it = (itr->second).find(word);
		if (it == (itr->second).end()) {
			found = false;
		}
	}

	if (found) pr = it->second;
	else pr = LOG10_PROB_ZERO;
	
	return found;
}

/* Given a bigram, if it exists, return its probability and bow information.
 * Note: LOG10_BOW_EMPTY may be returned if the bigram won't be a prefix or its frequency is too low.
 *       The probability and backoff weight of OOV will be LOG10_PROB_ZERO and LOG10_BOW_ZERO.
 */
bool LM::findBigramWithBOW(const string &history, const string &word, Log10Prob &pr, Log10BOW &bow) const {
	bool found = true;
	map<string, GramInfo>::const_iterator it;
	
	/* Get the existence of gram in the bigram map. */
	BigramWithBOW::const_iterator itr = bigramWithBOW.find(history);
	if (itr  == bigramWithBOW.end()) {
		found = false;
	} else {
		it = (itr->second).find(word);
		if (it == (itr->second).end()) {
			found = false;
		}
	}

	if (found) {
		pr = it->second.pr;
		bow = it->second.bow;
	} else {
		pr = LOG10_PROB_ZERO;
		bow = LOG10_BOW_ZERO;
	}

	return found;
}

/* Given a trigram, if it exists, return its probability, otherwise return LOG10_PROB_ZERO. */
bool LM::findTrigram(const string &dHistory, const string &history, const string &word, Log10Prob &pr) const {
	bool found = true;
	map<string, Log10Prob>::const_iterator i;

	/* Get the existence of gram in the trigram map.
	 * Note: Outer map:		itr->second.
	 *       Inner map:		it->second. 
	 *       Probability:	i->second.
	 */
	Trigram::const_iterator itr = trigram.find(dHistory);
	if (itr == trigram.end()) 	found = false;
    else {
		map<string, map<string, Log10Prob> >::const_iterator it = itr->second.find(history);
		if (it == itr->second.end()) found = false;
		else {
			i = it->second.find(word);
			if (i == it->second.end()) found = false;
		}
	}

	if (found) pr = i->second;
	else pr = LOG10_PROB_ZERO;
	return found;
}

/* -----------------------------------------Getter--------Functions---------------------------------------------------- */

/* LOG p(wd1) = if (wd1 is out of vocab)  LOG10_PROB_ZERO
 *			    else				      LOG p_1(wd1).
 *
 * Note: Though the probability of OOV is not 0 because of discounting, 
 *       but we don't know how many OOVs are, so it is of no interest to us and I assume its probability is 0.
 *       The program only supports the bigram and the trigram LM, 
 *	���� so the probability of a unigram is stored together with its backoff weight, the GramInfo structure.
 *       Both the bigram model and the trigram model are sharing the same structure. 
 */
Log10Prob LM::getLogUnigram(const string &word) const {
	Log10Prob pr, dummybow;
	return (!findUnigramWithBOW(word, pr, dummybow)) ? LOG10_PROB_ZERO : pr;
}

/* LOG p(wd2|wd1) = if (bigram exists)			     LOG p_2(wd1,wd2)
 *					if (wd1 or wd2 is out of vocab)	 LOG10_PROB_ZERO
 *                  else						     LOG bo_wt_1(wd1) + LOG p_1(wd2).
 *
 * Note: If both wd1 and wd2 are not OOVs, the probability of this bigram won't be 0 in the backoff model,
 *       for the probability or the backoff weight of any unigram within vocabulary would be non-zero.
 *       Getting the probability of a bigram from the trigram model is different from getting that from the bigram model,
 *	     for with the bigram model, it only stores the probability information without the backoff weight information.
 *	     But two cases can be merged.
 * Important note: findUnigramWithBOW(history, dummy, historyBOW) would get the same probability compared with getLogUnigram(const string &word),
 *                 which is different from that of bigram case.
 */
Log10Prob LM::getLogBigram(const string &history, const string &word) const {
	Log10Prob gramPr, historyBOW, wordPr, dummy;
	if (ORDER == 2 && findBigram(history, word, gramPr)) return gramPr;
	if (ORDER == 3 && findBigramWithBOW(history, word, gramPr, dummy)) return gramPr;
	if (!findUnigramWithBOW(word, wordPr, dummy)) return LOG10_PROB_ZERO;
	if (!findUnigramWithBOW(history, dummy, historyBOW)) return  LOG10_PROB_ZERO;
	return backoff(historyBOW, wordPr);
}

/* LOG p(wd3|wd1,wd2) = if(trigram exists)             LOG p_3(wd1,wd2,wd3)
 *                      else if(bigram w1,w2 exists)   LOG bo_wt_2(w1,w2) + LOG p(wd3|wd2)
 *                      else                           LOG p(wd3|w2)
 *
 * Note: If wd1, wd2 and wd3 are within vocabulary, the probability  > 0 after backoff.
 */
Log10Prob LM::getLogTrigram(const string &dHistory, const string &history, const string &word) const {
	if (ORDER == 2) {
		cerr << "Warning this is a bigram model, you are using getLogTrigram(). " << endl;
		return LOG10_PROB_ZERO;
	}

	Log10Prob bigramWordPr, historyBOW, gramPr, dummy;
	if (findTrigram(dHistory, history, word, gramPr)) return gramPr;

	bigramWordPr = getLogBigram(history, word);
	if (findBigramWithBOW(dHistory, history, dummy, historyBOW) && historyBOW != LOG10_BOW_EMPTY) 
		return backoff(historyBOW, bigramWordPr);
	else 
		return bigramWordPr;
}

Prob LM::getSentenceProb(const string &sentenceSeq) const {
	vector<string> sentence;
	segmentedStringToVector(sentenceSeq, sentence);

	Prob pr;
	if (ORDER == 2) {
		if (sentence.size() == 0) return 0;
		if (sentence.size() == 1) return getUnigram(sentence[0]);
		else {
			pr = getUnigram(sentence[0]);
			for (int i = 1; i < sentence.size(); i++) {
				pr *= getBigram(sentence[i - 1], sentence[i]);
			}
			return pr;
		}
	}
	if (ORDER == 3) {
		if (sentence.size() == 0) return 0;
		if (sentence.size() == 1) return getUnigram(sentence[0]);
		if (sentence.size() == 2) return getUnigram(sentence[0]) * getBigram(sentence[0], sentence[1]);
		else {
			pr = getUnigram(sentence[0]);
			pr *= getBigram(sentence[0], sentence[1]);
			for (int i = 2; i < sentence.size(); i++) {
				pr *= getTrigram(sentence[i - 2], sentence[i - 1], sentence[i]);
			}
			return pr;
		}
	}
	else return .0;
}

